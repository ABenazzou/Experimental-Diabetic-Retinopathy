{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport tensorflow as tf\n\nimport cv2\nimport os\n\nimport numpy as np\n\n!pip install split-folders\nimport splitfolders\n\nprint(\"Modules Imported Successfully\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-20T23:19:09.636052Z","iopub.execute_input":"2021-12-20T23:19:09.636413Z","iopub.status.idle":"2021-12-20T23:19:19.121697Z","shell.execute_reply.started":"2021-12-20T23:19:09.636378Z","shell.execute_reply":"2021-12-20T23:19:19.120354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reduce data so that we can do the workshops quickly\n#os.makedirs('DR/train')\n#os.makedirs('DR/test')\n#divide train data onto 8% train 2% test\ninitial_dataset = '../input/diabetic-retinopathy-balanced/content/Diabetic_Balanced_Data/train'\nsplitfolders.ratio(initial_dataset, output='DR', seed=1337, ratio=(0.08, 0.9, 0.02))\nprint(\"Directories Created Successfully\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:16:02.827774Z","iopub.execute_input":"2021-12-20T23:16:02.828102Z","iopub.status.idle":"2021-12-20T23:17:08.742605Z","shell.execute_reply.started":"2021-12-20T23:16:02.828059Z","shell.execute_reply":"2021-12-20T23:17:08.741147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = 'DR/train';\ntest_path = 'DR/test';\nprint(\"Number of Classes in test data: \", len(os.listdir(train_path)))\nprint(\"Number of Classes in test data: \", len(os.listdir(test_path)))","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:21:36.497593Z","iopub.execute_input":"2021-12-20T23:21:36.498471Z","iopub.status.idle":"2021-12-20T23:21:36.506943Z","shell.execute_reply.started":"2021-12-20T23:21:36.498418Z","shell.execute_reply":"2021-12-20T23:21:36.505907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clahe = cv2.createCLAHE(clipLimit = 5)#Contrast Limited Adaptive Histogram Equalization\nimg_name = '10010_right.jpeg'\nimg_normal = cv2.imread(train_path + \"/0/\" + img_name)\nimg_normal = cv2.resize(img_normal, (224,224))\nimg_normal = cv2.cvtColor(img_normal, cv2.COLOR_BGR2GRAY)\nimg_normal_clahe = clahe.apply(img_normal) + 30\n\n\nimg_name_1 = '10312_left.jpeg'\nimg_proliferative = cv2.imread(train_path + \"/4/\" + img_name_1)\nimg_proliferative = cv2.resize(img_proliferative, (224,224))\nimg_proliferative = cv2.cvtColor(img_proliferative, cv2.COLOR_BGR2GRAY)\nimg_proliferative_clahe = clahe.apply(img_proliferative) + 30\n\n\n\n\nfig, axs = plt.subplots(2,2,figsize=(10,6))\naxs[0,0].imshow(img_normal)\naxs[0,0].set_title(\"NORMAL\")\naxs[0,1].imshow(img_normal_clahe)\naxs[0,1].set_title(\"NORMAL_CLAHE\")\naxs[1,0].imshow(img_proliferative)\naxs[1,0].set_title(\"PROLIFERATIVE\")\naxs[1,1].imshow(img_proliferative_clahe)\naxs[1,1].set_title(\"PROLIFERATIVE_CLAHE\");","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:27:33.956312Z","iopub.execute_input":"2021-12-20T23:27:33.956664Z","iopub.status.idle":"2021-12-20T23:27:34.665541Z","shell.execute_reply.started":"2021-12-20T23:27:33.956612Z","shell.execute_reply":"2021-12-20T23:27:34.664794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Kerasâ€™ ImageDataGenerator class allows to implement a solution where images are loaded \n#and augmented in batch as the training progresses.\nimg_width , img_height = [224,224]\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    brightness_range=[0.2,1.0],\n    horizontal_flip=True)\n\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle = True)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_path,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:27:41.176749Z","iopub.execute_input":"2021-12-20T23:27:41.177197Z","iopub.status.idle":"2021-12-20T23:27:41.502844Z","shell.execute_reply.started":"2021-12-20T23:27:41.177161Z","shell.execute_reply":"2021-12-20T23:27:41.501700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_batch, label_batch = next(iter(train_generator))\n\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10, 10))\n    for n in range(15):\n        ax = plt.subplot(5, 5, n + 1)\n        plt.imshow(image_batch[n])\n        #print(list(label_batch[n]))\n        #print(list(label_batch[n]).index(1.0))\n        if list(label_batch[n]).index(1) == 0:\n            plt.title(\"Normal\")\n        elif list(label_batch[n]).index(1) == 1:\n            plt.title(\"Mild\")\n        elif list(label_batch[n]).index(1) == 2:\n            plt.title(\"Moderate\")\n        elif list(label_batch[n]).index(1) == 3:\n            plt.title(\"Severe\")\n        else:\n            plt.title(\"Proliferative\")\n        plt.axis(\"off\")\n\nshow_batch(image_batch, label_batch)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:27:44.992487Z","iopub.execute_input":"2021-12-20T23:27:44.992810Z","iopub.status.idle":"2021-12-20T23:27:46.485019Z","shell.execute_reply.started":"2021-12-20T23:27:44.992777Z","shell.execute_reply":"2021-12-20T23:27:46.483684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nb_train_samples = 34792 # number of training samples\nnb_train_samples = 2783\nnb_test_samples = 697\n#nb_test_samples = 4971 # number of training samples\nepochs = 3  # number of epochs we gonna run\nbatch_size  = 32 # batch size ( at every iteration it will take 32 batches for training)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:27:59.296432Z","iopub.execute_input":"2021-12-20T23:27:59.296945Z","iopub.status.idle":"2021-12-20T23:27:59.301783Z","shell.execute_reply.started":"2021-12-20T23:27:59.296904Z","shell.execute_reply":"2021-12-20T23:27:59.300956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation = \"relu\", input_shape = (224,224,3)))\n#32 filters 3x3 filter size\nmodel.add(MaxPool2D())\n#relu eliminates negative vals, max pooling reduces size of input\n\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation = \"relu\"))\nmodel.add(MaxPool2D())\n\nmodel.add(Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\"))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.4))#avoid overfitting\n\nmodel.add(Flatten()) \nmodel.add(Dense(128, activation=\"relu\"))\nmodel.add(Dense(5, activation=\"softmax\"))\n#softmax for alot of classes\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:28:05.246301Z","iopub.execute_input":"2021-12-20T23:28:05.246837Z","iopub.status.idle":"2021-12-20T23:28:05.364130Z","shell.execute_reply.started":"2021-12-20T23:28:05.246794Z","shell.execute_reply":"2021-12-20T23:28:05.363035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = Adam(lr=0.000001)\n#Adam replaces gradient descent\nmodel.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nEarlyStopping = EarlyStopping(monitor='val_accuracy',\n                              min_delta=.01,\n                              patience=7,\n                              verbose=1,\n                              mode='max',\n                              baseline=None,\n                              restore_best_weights=True)\n#Reduce learning rate when a metric has stopped improving.\nrlr = ReduceLROnPlateau( monitor=\"val_accuracy\",\n                            factor=0.01,\n                            patience=3,\n                            verbose=0,\n                            mode=\"max\",\n                            min_delta=0.01)\n\nmodel_save = ModelCheckpoint('./Model.h5',\n                             save_best_only = True,\n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch = nb_train_samples // batch_size, epochs = 3, validation_data = test_generator, callbacks=[EarlyStopping, model_save,rlr])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# its always a good practice to load the model after saving with the best epochs \nmodel = keras.models.load_model('./Model.h5')\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(3)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(test_generator)\npredictions = np.argmax(predict_x, axis = 1)\npredictions = predictions.reshape(1,-1)[0]\nprint(classification_report(test_generator.classes, predictions, \n                            target_names = ['No DR (Class 0)','Mild (Class 1)',\n                                            'Moderate (Class 2)', 'Severe (Class 3)',\n                                            'Proliferative DR (Class 4)']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transfer Learning Part\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape = (224, 224, 3),\n                                               include_top = False,\n                                               weights = \"imagenet\")\n#prevent weight update while training\nfor layer in base_model.layers:\n    layer.trainable =  False\n#add our layers\nmodel = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalAveragePooling2D(),\n                                 tf.keras.layers.Dropout(0.2),\n                                 tf.keras.layers.Dense(5, activation=\"softmax\")                                     \n                                ])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nopt = Adam(lr=0.000001)\n#Adam replaces gradient descent\nmodel.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping,ReduceLROnPlateau\nEarlyStopping = EarlyStopping(monitor='val_accuracy',\n                              min_delta=.01,\n                              patience=6,\n                              verbose=1,\n                              mode='auto',\n                              baseline=None,\n                              restore_best_weights=True)\n\nrlr = ReduceLROnPlateau( monitor=\"val_accuracy\",\n                            factor=0.01,\n                            patience=6,\n                            verbose=0,\n                            mode=\"max\",\n                            min_delta=0.01)\n\nmodel_save = ModelCheckpoint('./Model2.h5',\n                             save_best_only = True,\n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch = nb_train_samples // batch_size, epochs = 3, validation_data = test_generator, callbacks=[EarlyStopping, model_save,rlr])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# its always a good practice to load the model after saving with the best epochs \nmodel = keras.models.load_model('./Model2.h5')\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(3)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(test_generator)\npredictions = np.argmax(predict_x, axis = 1)\npredictions = predictions.reshape(1,-1)[0]\nprint(classification_report(test_generator.classes, predictions, \n                            target_names = ['No DR (Class 0)','Mild (Class 1)',\n                                            'Moderate (Class 2)', 'Severe (Class 3)',\n                                            'Proliferative DR (Class 4)']))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transfer Learning Part\n\nbase_model = tf.keras.applications.VGG16(input_shape = (224, 224, 3),\n                                               include_top = False,\n                                               weights = \"imagenet\")\n#prevent weight update while training\nfor layer in base_model.layers:\n    layer.trainable =  False\n#add our layers\nmodel = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalMaxPooling2D(),\n                                 tf.keras.layers.Dropout(0.2),\n                                 tf.keras.layers.Dense(5, activation=\"softmax\")                                     \n                                ])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nopt = Adam(lr=0.000001)\n#Adam replaces gradient descent\nmodel.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nEarlyStopping = EarlyStopping(monitor='val_accuracy',\n                              min_delta=.01,\n                              patience=6,\n                              verbose=1,\n                              mode='auto',\n                              baseline=None,\n                              restore_best_weights=True)\n\nrlr = ReduceLROnPlateau( monitor=\"val_accuracy\",\n                            factor=0.01,\n                            patience=6,\n                            verbose=0,\n                            mode=\"max\",\n                            min_delta=0.01)\n\nmodel_save = ModelCheckpoint('./Model3.h5',\n                             save_best_only = True,\n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch = nb_train_samples // batch_size, epochs = 3, validation_data = test_generator, callbacks=[EarlyStopping, model_save,rlr])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# its always a good practice to load the model after saving with the best epochs \nmodel = keras.models.load_model('./Model3.h5')\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(3)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(test_generator)\npredictions = np.argmax(predict_x, axis = 1)\npredictions = predictions.reshape(1,-1)[0]\nprint(classification_report(test_generator.classes, predictions, \n                            target_names = ['No DR (Class 0)','Mild (Class 1)',\n                                            'Moderate (Class 2)', 'Severe (Class 3)',\n                                            'Proliferative DR (Class 4)']))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transfer Learning Part\n\nbase_model = tf.keras.applications.VGG19(input_shape = (224, 224, 3),\n                                               include_top = False,\n                                               weights = \"imagenet\")\n#prevent weight update while training\nfor layer in base_model.layers:\n    layer.trainable =  False\n#add our layers\nmodel = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalMaxPooling2D(),\n                                 tf.keras.layers.Dropout(0.2),\n                                 tf.keras.layers.Dense(5, activation=\"softmax\")                                     \n                                ])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nopt = Adam(lr=0.000001)\n#Adam replaces gradient descent\nmodel.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\nEarlyStopping = EarlyStopping(monitor='val_accuracy',\n                              min_delta=.01,\n                              patience=6,\n                              verbose=1,\n                              mode='auto',\n                              baseline=None,\n                              restore_best_weights=True)\n\nrlr = ReduceLROnPlateau( monitor=\"val_accuracy\",\n                            factor=0.01,\n                            patience=6,\n                            verbose=0,\n                            mode=\"max\",\n                            min_delta=0.01)\n\nmodel_save = ModelCheckpoint('./Model4.h5',\n                             save_best_only = True,\n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch = nb_train_samples // batch_size, epochs = 3, validation_data = test_generator, callbacks=[EarlyStopping, model_save,rlr])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# its always a good practice to load the model after saving with the best epochs \nmodel = keras.models.load_model('./Model4.h5')\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(3)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(test_generator)\npredictions = np.argmax(predict_x, axis = 1)\npredictions = predictions.reshape(1,-1)[0]\nprint(classification_report(test_generator.classes, predictions, \n                            target_names = ['No DR (Class 0)','Mild (Class 1)',\n                                            'Moderate (Class 2)', 'Severe (Class 3)',\n                                            'Proliferative DR (Class 4)']))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transfer Learning Part\n\nbase_model = tf.keras.applications.Xception(input_shape = (224, 224, 3),\n                                               include_top = False,\n                                               weights = \"imagenet\")\n#prevent weight update while training\nfor layer in base_model.layers:\n    layer.trainable =  False\n#add our layers\nmodel = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalMaxPooling2D(),\n                                 tf.keras.layers.Dropout(0.2),\n                                 tf.keras.layers.Dense(5, activation=\"softmax\")                                     \n                                ])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in base_model.layers:\n    layer.trainable =  False\n#add our layers\nmodel = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalMaxPooling2D(),\n                                 tf.keras.layers.Dropout(0.2),\n                                 tf.keras.layers.Dense(5, activation=\"softmax\")                                     \n                                ])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nopt = Adam(lr=0.000001)\n#Adam replaces gradient descent\nmodel.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\nEarlyStopping = EarlyStopping(monitor='val_accuracy',\n                              min_delta=.01,\n                              patience=6,\n                              verbose=1,\n                              mode='auto',\n                              baseline=None,\n                              restore_best_weights=True)\n\nrlr = ReduceLROnPlateau( monitor=\"val_accuracy\",\n                            factor=0.01,\n                            patience=6,\n                            verbose=0,\n                            mode=\"max\",\n                            min_delta=0.01)\n\nmodel_save = ModelCheckpoint('./Model5.h5',\n                             save_best_only = True,\n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch = nb_train_samples // batch_size, epochs = 3, validation_data = test_generator, callbacks=[EarlyStopping, model_save,rlr])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# its always a good practice to load the model after saving with the best epochs \nmodel = keras.models.load_model('./Model5.h5')\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(3)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(test_generator)\npredictions = np.argmax(predict_x, axis = 1)\npredictions = predictions.reshape(1,-1)[0]\nprint(classification_report(test_generator.classes, predictions, \n                            target_names = ['No DR (Class 0)','Mild (Class 1)',\n                                            'Moderate (Class 2)', 'Severe (Class 3)',\n                                            'Proliferative DR (Class 4)']))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transfer Learning Part\n\nbase_model = tf.keras.applications.InceptionV3(input_shape = (224, 224, 3),\n                                               include_top = False,\n                                               weights = \"imagenet\")\n#prevent weight update while training\nfor layer in base_model.layers:\n    layer.trainable =  False\n#add our layers\nmodel = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalMaxPooling2D(),\n                                 tf.keras.layers.Dropout(0.2),\n                                 tf.keras.layers.Dense(5, activation=\"softmax\")                                     \n                                ])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:29:00.699769Z","iopub.execute_input":"2021-12-20T23:29:00.700060Z","iopub.status.idle":"2021-12-20T23:29:04.260994Z","shell.execute_reply.started":"2021-12-20T23:29:00.700027Z","shell.execute_reply":"2021-12-20T23:29:04.259729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nopt = Adam(lr=0.001)\n#Adam replaces gradient descent\nmodel.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:29:07.936914Z","iopub.execute_input":"2021-12-20T23:29:07.937297Z","iopub.status.idle":"2021-12-20T23:29:07.954220Z","shell.execute_reply.started":"2021-12-20T23:29:07.937258Z","shell.execute_reply":"2021-12-20T23:29:07.953239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\nEarlyStopping = EarlyStopping(monitor='val_accuracy',\n                              min_delta=.01,\n                              patience=6,\n                              verbose=1,\n                              mode='auto',\n                              baseline=None,\n                              restore_best_weights=True)\n\nrlr = ReduceLROnPlateau( monitor=\"val_accuracy\",\n                            factor=0.01,\n                            patience=6,\n                            verbose=0,\n                            mode=\"max\",\n                            min_delta=0.01)\n\nmodel_save = ModelCheckpoint('./Model6.h5',\n                             save_best_only = True,\n                             save_weights_only = False,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\n ","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:29:10.330041Z","iopub.execute_input":"2021-12-20T23:29:10.330382Z","iopub.status.idle":"2021-12-20T23:29:10.338354Z","shell.execute_reply.started":"2021-12-20T23:29:10.330346Z","shell.execute_reply":"2021-12-20T23:29:10.337268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, steps_per_epoch = nb_train_samples // batch_size, epochs = 3, validation_data = test_generator, callbacks=[EarlyStopping, model_save,rlr])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:29:13.168981Z","iopub.execute_input":"2021-12-20T23:29:13.169284Z","iopub.status.idle":"2021-12-20T23:39:40.138424Z","shell.execute_reply.started":"2021-12-20T23:29:13.169250Z","shell.execute_reply":"2021-12-20T23:39:40.137261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# its always a good practice to load the model after saving with the best epochs \nmodel = keras.models.load_model('./Model6.h5')\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(3)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label = 'Training Accuracy')\nplt.plot(epochs_range, val_acc, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:39:46.703684Z","iopub.execute_input":"2021-12-20T23:39:46.704053Z","iopub.status.idle":"2021-12-20T23:39:50.645609Z","shell.execute_reply.started":"2021-12-20T23:39:46.704012Z","shell.execute_reply":"2021-12-20T23:39:50.644418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_x = model.predict(test_generator)\npredictions = np.argmax(predict_x, axis = 1)\npredictions = predictions.reshape(1,-1)[0]\nprint(classification_report(test_generator.classes, predictions, \n                            target_names = ['No DR (Class 0)','Mild (Class 1)',\n                                            'Moderate (Class 2)', 'Severe (Class 3)',\n                                            'Proliferative DR (Class 4)']))","metadata":{"execution":{"iopub.status.busy":"2021-12-20T23:40:07.953082Z","iopub.execute_input":"2021-12-20T23:40:07.953427Z","iopub.status.idle":"2021-12-20T23:40:47.022947Z","shell.execute_reply.started":"2021-12-20T23:40:07.953388Z","shell.execute_reply":"2021-12-20T23:40:47.021732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_image_adversary(model, image, label, eps=2 / 255.0):#model to fool, image to missclass nd label\n    #eps small to fool comp not the human eye\n    # cast the image\n    image = tf.cast(image, tf.float32)\n    with tf.GradientTape() as tape:\n        # explicitly indicate that our image should be tacked for\n        # gradient updates\n        tape.watch(image)\n        # use our model to make predictions on the input image and then compute the loss\n        pred = model.predict(image)\n       # print(pred)\n        loss = tf.keras.losses.CategoricalCrossentropy(label, pred)\n        # calculate the gradients of loss with respect to the image, then\n        # compute the sign of the gradient\n        gradient = tape.gradient(loss, image)\n        signedGrad = tf.sign(gradient)\n        # construct the image adversary\n        adversary = (image + (signedGrad * eps)).numpy()\n        # return the image adversary to the calling function\n        return adversary","metadata":{"execution":{"iopub.status.busy":"2021-12-21T01:03:52.174390Z","iopub.execute_input":"2021-12-21T01:03:52.175266Z","iopub.status.idle":"2021-12-21T01:03:52.184455Z","shell.execute_reply.started":"2021-12-21T01:03:52.175210Z","shell.execute_reply":"2021-12-21T01:03:52.183642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loop over a sample of our testing images\nx,y = train_generator.next()\nfor i in np.random.choice(np.arange(0, len(x)), size=(10,)):\n    # grab the current image and label\n    image = x[i]\n    #plt.imshow(image)\n    #print(type(image))\n    label = y[i].tolist().index(1)\n    # generate an image adversary for the current image and make\n    # a prediction on the adversary\n    image = image.reshape(-1, 224, 224, 3)\n    image = tf.image.convert_image_dtype(image, dtype = tf.float32)\n    adversary = generate_image_adversary(model,image, label, eps=0.1)\n    pred = model.predict(adversary)\n   ","metadata":{"execution":{"iopub.status.busy":"2021-12-21T01:18:49.278089Z","iopub.execute_input":"2021-12-21T01:18:49.278561Z","iopub.status.idle":"2021-12-21T01:18:51.510945Z","shell.execute_reply.started":"2021-12-21T01:18:49.278518Z","shell.execute_reply":"2021-12-21T01:18:51.509495Z"},"trusted":true},"execution_count":null,"outputs":[]}]}